{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31632f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1455bcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"eating\",\"eats\",\"eat\",\"ate\",\"adjustable\",\"rafting\",\"ability\",\"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"|\",stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6659f",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. \n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as the same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n",
    "Applications of lemmatization are: \n",
    " \n",
    "\n",
    "Used in comprehensive retrieval systems like search engines.\n",
    "Used in compact indexing\n",
    " \n",
    "\n",
    "Examples of lemmatization:\n",
    "\n",
    "-> rocks : rock\n",
    "-> corpora : corpus\n",
    "-> better : good\n",
    "\n",
    "One major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”\n",
    "Below is the implementation of lemmatization words using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d58584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda32437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "eating : eating\n",
      "better : better\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\",lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"eating :\",lemmatizer.lemmatize(\"eating\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\"))\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1f4de",
   "metadata": {},
   "source": [
    "\n",
    "# Python | PoS Tagging and Lemmatization using spaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f27c7",
   "metadata": {},
   "source": [
    "spaCy is one of the best text analysis library. spaCy excels at large-scale information extraction tasks and is one of the fastest in the world. It is also the best way to prepare text for deep learning. spaCy is much faster and accurate than NLTKTagger and TextBlob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5270d8",
   "metadata": {},
   "source": [
    "Top Features of spaCy:\n",
    "1. Non-destructive tokenization\n",
    "2. Named entity recognition\n",
    "3. Support for 49+ languages\n",
    "4. 16 statistical models for 9 languages\n",
    "5. Pre-trained word vectors\n",
    "6. Part-of-speech tagging\n",
    "7. Labeled dependency parsing\n",
    "8. Syntax-driven sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d748de06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for matplotlib: [Errno 2] No such file or directory: '/home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages/matplotlib-3.5.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m2023-02-28 12:07:31.602198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 12:07:31.700975: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-28 12:07:32.165128: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 12:07:32.165191: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 12:07:32.165197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-28 12:07:33.013112: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-28 12:07:33.013129: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-28 12:07:33.013149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nsl47): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (63.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for matplotlib: [Errno 2] No such file or directory: '/home/nsl47/anaconda3/envs/bd/lib/python3.10/site-packages/matplotlib-3.5.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23498ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PRON\n",
      "name NOUN\n",
      "is AUX\n",
      "Shaurya PROPN\n",
      "Uppal PROPN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n",
      "I PRON\n",
      "enjoy VERB\n",
      "writing VERB\n",
      "articles NOUN\n",
      "on ADP\n",
      "GeeksforGeeks PROPN\n",
      "checkout VERB\n",
      "\n",
      " SPACE\n",
      "my PRON\n",
      "other ADJ\n",
      "article NOUN\n",
      "by ADP\n",
      "going VERB\n",
      "to ADP\n",
      "my PRON\n",
      "profile NOUN\n",
      "section NOUN\n",
      ". PUNCT\n",
      "Verbs : ['enjoy', 'writing', 'checkout', 'going']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, \n",
    "# parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"\"\"My name is Shaurya Uppal. \n",
    "I enjoy writing articles on GeeksforGeeks checkout\n",
    "my other article by going to my profile section.\"\"\")\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Token and tag\n",
    "for token in doc:\n",
    "    print(token,token.pos_)\n",
    "# You web list of verb tokens\n",
    "print(\"Verbs :\",[token.text for token in doc if token.pos_ == \"VERB\" ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c06d437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "#Customizing lemmatizer\n",
    "nlp.pipe_names\n",
    "\n",
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a932dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
